# 问题汇总

[TOC]

### 问题1: 权限问题

在使用 sqoop2，从 mysql 传递数据到 hdfs 中时:

(1)出现 `Caused by: Exception: java.sql.SQLException Message: Access denied for user 'root'@'zgg' (using password: YES`

需要创建用户，并赋予权限

```sql
mysql> CREATE USER 'root'@'zgg' IDENTIFIED BY '1234';               
Query OK, 0 rows affected (0.02 sec)

mysql> grant all privileges on *.* to 'root'@'zgg';
Query OK, 0 rows affected (0.03 sec)

mysql> flush privileges;
Query OK, 0 rows affected (0.01 sec)
```

(2)出现 `User: root is not allowed to impersonate root` 错误。

在 `core-site.xml` 里，将"xxx"替换成root即可，再重启hdfs

```xml
<property>
        <name>hadoop.proxyuser.xxx.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.xxx.groups</name>
        <value>*</value>
    </property>
```

`*` 表示可通过超级代理 "xxx" 操作 hadoop 的用户、用户组和主机

(3)出现 `ClassNotFoundException: org.apache.commons.lang.StringUtils`

在 lib 目录下，分别测试放了 `commons-lang-2.6.jar` 和 `commons-lang3-3.11.jar`。还是出现问题。

【未解决】

### 问题2: 版本问题

sqoop 版本更换为 1.4.7 时:

(1)出现了 `ClassNotFoundException: org.apache.commons.lang.StringUtils`问题，把lib目录下的`commons-lang3-3.4.jar ` 换成了 `commons-lang-2.6.jar`，问题解决。


(2)出现 `ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf`，只需 `cp hive-common-3.1.2.jar /opt/sqoop-1.4.7.bin__hadoop-2.6.0/lib`


### 问题3: 字段类型1

如果字段在数据表中的数据类型为datetime，导入hive中后，会转换为string类型。

可以增加选项 `--map-column-hive 字段名=新数据类型`，在映射到hive中时，将字段的数据类型修改为对应的类型。

所以，在导入中，字段类型要对应。

```sql
mysql> desc apps_datetime;
+-------+-------------+------+-----+---------+-------+
| Field | Type        | Null | Key | Default | Extra |
+-------+-------------+------+-----+---------+-------+
| id    | int         | NO   | PRI | NULL    |       |
| name  | varchar(20) | NO   |     | NULL    |       |
| date  | datetime    | YES  |     | NULL    |       |
+-------+-------------+------+-----+---------+-------+

mysql> select * from apps_datetime;
+----+------+---------------------+
| id | name | date                |
+----+------+---------------------+
|  1 | aa   | 2020-01-01 10:10:10 |
|  2 | bb   | 2020-01-02 12:12:12 |
|  3 | cc   | 2020-01-03 13:13:13 |
+----+------+---------------------+

# 导入
[root@zgg sqoop-1.4.7.bin__hadoop-2.6.0]# sqoop import --connect jdbc:mysql://zgg:3306/users --driver com.mysql.cj.jdbc.Driver --username root --password 1234 --table apps_datetime --hive-import --target-dir '/user/hive/warehouse/apps_datetime' --fields-terminated-by ',' --lines-terminated-by '\n'

-- 导入后，hive中查看，转成了string类型，添加了`.0`
hive> desc apps_datetime;
OK
id                      int                                         
name                    string                                      
date                    string                                      

hive> select * from apps_datetime;
OK
1       aa      2020-01-02 00:10:10.0
2       bb      2020-01-03 02:12:12.0
3       cc      2020-01-04 03:13:13.0

# 增加`--map-column-hive`选项，导入
[root@zgg sqoop-1.4.7.bin__hadoop-2.6.0]# sqoop import --connect jdbc:mysql://zgg:3306/users --driver com.mysql.cj.jdbc.Driver --username root --password 1234 --table apps_datetime --hive-import --target-dir '/user/hive/warehouse/apps_datetime' --map-column-hive date=timestamp --fields-terminated-by ',' --lines-terminated-by '\n'

-- 导入后，hive中查看
hive> desc apps_datetime;
OK
id                      int                                         
name                    string                                      
date                    timestamp                                   

hive> select * from apps_datetime;
OK
1       aa      2020-01-02 00:10:10
2       bb      2020-01-03 02:12:12
3       cc      2020-01-04 03:13:13
```

### 问题4: 字段类型2

如果数据表中包含blob或text类型的字段，里面有各种字符，其中就包括hive中默认的分隔符和自定义的分隔符。这样当导数据到hive中就会造成数据混乱。

可以替换成空字符串，或直接删除：

    --hive-delims-replacement
    --hive-drop-import-delims

```sql
-- text类型
mysql> desc apps_text;         
+-------+------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-------+------+------+-----+---------+-------+
| id    | int  | NO   | PRI | NULL    |       |
| name  | text | NO   |     | NULL    |       |
+-------+------+------+-----+---------+-------+

mysql> select * from apps_text;
+----+-------+
| id | name  |
+----+-------+
|  1 | aa
bb |
|  2 | cc
dd |
+----+-------+
mysql> select * from apps_text where id=1;
+----+-------+
| id | name  |
+----+-------+
|  1 | aa
bb |
+----+-------+

# 导入
[root@zgg sqoop-1.4.7.bin__hadoop-2.6.0]# sqoop import --connect jdbc:mysql://zgg:3306/users --driver com.mysql.cj.jdbc.Driver --username root --password 1234 --table apps_text --hive-import --target-dir '/user/hive/warehouse/apps_text' --fields-terminated-by ',' --lines-terminated-by '\n'

-- 导入后，hive中查看
hive> select * from apps_text;
OK
1       aa
NULL    NULL
2       cc
NULL    NULL

hive> desc apps_text;
OK
id                      int                                         
name                    string 

hive> select * from apps_text where id=1;
OK
1       aa

# 添加`--hive-delims-replacement` 
# Replace \n, \r, and \01 from string fields with user defined string when importing to Hive.
[root@zgg sqoop-1.4.7.bin__hadoop-2.6.0]# sqoop import --connect jdbc:mysql://zgg:3306/users --driver com.mysql.cj.jdbc.Driver --username root --password 1234 --table apps_text --hive-import --target-dir '/user/hive/warehouse/apps_text' --hive-delims-replacement '' --fields-terminated-by ',' --lines-terminated-by '\n'

-- 导入后，hive中查看
hive> select * from apps_text;
OK
1       aabb
2       ccdd
hive> desc apps_text;
OK
id                      int                                         
name                    string

# 添加`--hive-drop-import-delims` 
# Drops \n, \r, and \01 from string fields when importing to Hive.
[root@zgg sqoop-1.4.7.bin__hadoop-2.6.0]# sqoop import --connect jdbc:mysql://zgg:3306/users --driver com.mysql.cj.jdbc.Driver --username root --password 1234 --table apps_text --hive-import --target-dir '/user/hive/warehouse/apps_text' --hive-drop-import-delims --fields-terminated-by ',' --lines-terminated-by '\n'

-- 导入后，hive中查看
hive> select * from apps_text;
OK
1       aabb
2       ccdd
hive> desc apps_text;
OK
id                      int                                         
name                    string
```

**所以，在导入hive中，要熟悉原表的结构，熟悉字段的数据类型，结合sqoop命令的参数选项**

## 问题5: blob处理

sqoop将表导入hive时，如果表里有blob类型，如果直接导入，会出现错误：`ERROR tool.ImportTool: Import failed: java.io.IOException: Hive does not support the SQL type for column name`

（1）方法1: 在HIVE中禁用blob字段

oraoop-site.xml

```xml
  <property>
    <name>oraoop.import.omit.lobs.and.long</name>
    <value>true</value>
    <description>If true, OraOop will omit BLOB, CLOB, NCLOB and LONG columns during an Import.
    </description>
  </property>
```

```sql
mysql> select * from apps_blob;
+----+--------------+
| id | name         |
+----+--------------+
|  1 | 0x6161096262 |
|  2 | 0x6363096464 |
+----+--------------+

[root@zgg sqoop-1.4.7.bin__hadoop-2.6.0]# sqoop import --connect jdbc:mysql://zgg:3306/users --driver com.mysql.cj.jdbc.Driver --username root --password 1234 --table apps_blob --hive-import --target-dir '/user/hive/warehouse/apps_blob' --hive-drop-import-delims --map-column-hive name=string --fields-terminated-by ',' --lines-terminated-by '\n'  

hive> select * from apps_blob;
OK
1       61 61 09 62 62
2       63 63 09 64 64
```

（2）方法2: 当不能禁用时，如果需要其他字段，而不需要blob字段，所以在导入的时候指定`--columns`来过滤。

```sql
mysql> alter table apps_blob add age int(10) not null;  
mysql> select * from apps_blob;
+----+--------------+-----+
| id | name         | age |
+----+--------------+-----+
|  1 | 0x6161096262 |  11 |
|  2 | 0x6363096464 |  22 |
+----+--------------+-----+

mysql> desc apps_blob;
+-------+------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-------+------+------+-----+---------+-------+
| id    | int  | NO   | PRI | NULL    |       |
| name  | blob | NO   |     | NULL    |       |
| age   | int  | NO   |     | NULL    |       |
+-------+------+------+-----+---------+-------+

[root@zgg sqoop-1.4.7.bin__hadoop-2.6.0]# sqoop import --connect jdbc:mysql://zgg:3306/users --driver com.mysql.cj.jdbc.Driver --username root --password 1234 --table apps_blob --hive-import --target-dir '/user/hive/warehouse/apps_blob' --columns 'id,age' --fields-terminated-by ',' --lines-terminated-by '\n' 

hive> select * from apps_blob;
OK
1       11
2       22
3       33

hive> desc apps_blob;
OK
id                      int                                         
age                     int 
```

（3）方法3: 入库后，再使用 udf 来转换

blob字段导入到hdfs后显示为16进制，那么就可以编写 udf 将16进制转换为 string

（4）方法4: 从数据库导出时，把 blob 转成 string 保存

```
--map-column-java [字段名]=String
--map-column-hive [字段名]=String
```

对blob类型的处理的讨论：[https://issues.apache.org/jira/browse/HIVE-637](https://issues.apache.org/jira/browse/HIVE-637)
